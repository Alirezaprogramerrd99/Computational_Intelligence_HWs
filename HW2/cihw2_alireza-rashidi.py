# -*- coding: utf-8 -*-
"""CIHW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXJSKQg0JILHSyDfszbF4V7PoCpKUrYw

# CIHW-2 Alireza Rashidi

## Preparing Dataset files:
"""

from google.colab import drive
drive.mount('/content/gdrive')

!cp /content/gdrive/MyDrive/Datasets/Dataset.zip /content/sample_data

!mkdir /content/CIHW2_Datasets
!unzip /content/sample_data/Dataset.zip -d /content/CIHW2_Datasets

"""## Understanding of Dataset:"""

import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import math

train_df = pd.read_csv('/content/CIHW2_Datasets/train.csv')
train_df.head(10)

print(train_df.shape)

print('shape of dataset: ', train_df.shape)
print('\ncol names: ', train_df.columns)
print('\nnumber of columns: ', len(train_df.columns))
print('\ncolomns data types:\n', train_df.dtypes)
# print('\nmissing values:\n', train_df.isnull().sum())
print(" \nCount total missing values in a DataFrame: ", train_df.isnull().sum().sum())

"""### Seeing the number of missing values in percent:"""

total = train_df.isnull().sum().sort_values(ascending=False)
percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100
percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
missing_data = pd.concat([total, percent_2], axis=1, keys=['Total missing values', 'In percent %'])
missing_data

train_df.describe()

"""## Data Preprocessing:

### Handling Missing data and data cleaning:
"""

"""At first, we delete cabin column becuase it has so much missing data and we can't do any thing to fix this problem, e.g: data imputation. 
   Also we delete passenger's ID from dataset beacuse it cuases the model to overfit due to uniqueness of this columns's values and it does not contribute to a persons survival probability.
   Name column is also like passenger's ID.
"""
train_df = train_df.drop(['Cabin', 'PassengerId', 'Name', 'Ticket'], axis = 1)
print('shape of dataset: ', train_df.shape)
print('\ncol names: ', train_df.columns)

"""### Handling Age column missing data and catagorize it's values:"""

"""About Age feature, we use data imputation beacuse we have some information about this column. """
train_df['Age'] = train_df['Age'].replace(np.NaN, train_df['Age'].mean() )
train_df['Age']

# creating categories for age column.
train_df.loc[ train_df['Age'] <= 19, 'Age'] = 0
train_df.loc[(train_df['Age'] > 19) & (train_df['Age'] <= 25), 'Age'] = 1
train_df.loc[(train_df['Age'] > 25) & (train_df['Age'] <= 31.8), 'Age'] = 2
train_df.loc[(train_df['Age'] > 31.8) & (train_df['Age'] <= 41), 'Age'] = 3
train_df.loc[train_df['Age'] > 41, 'Age'] = 4
    
train_df['Age']

# we will print number of samples per each catagory.
train_df['Age'].value_counts()

"""#### Handeling Embarked colomn missing data: """

train_df['Embarked'].describe()

""" due to freaquency of 'S' in this column and becasue we have 2 missing data, we replace them with most freaquent one."""
train_df['Embarked'] = train_df['Embarked'].fillna('S')       # fillna fills null or missing values with specific value.
train_df['Embarked'].describe()

"""### Encoding and converting catagorical values to numerical values:

Usually Algorithems Work better with numerical values, so we must convert this catagorical values to numerical values to achive maximum accuracy
"""

Embarked_encodings = {'S': 0, 'C': 1, 'Q': 2}
gender_encodings = {'male' : 0,'female' : 1}
train_df['Embarked'] = train_df['Embarked'].map(Embarked_encodings)
train_df['Sex'] = train_df['Sex'].map(gender_encodings)

"""As shown in below statement, we converted all of colomns to numrical data that we can use them to train models and fixed missing data problem:"""

print('\ncolomns data types:\n', train_df.dtypes)
print('\nmissing values:\n', train_df.isnull().sum())

"""## Spliting features and labels from dataset:"""

train_y_df = train_df.iloc[:, 1]   # all rows from col number 1.
train_X_df = train_df.drop(['Survived'], axis = 1)  # spliting Survived column from features dataset.
print(train_X_df.shape, train_y_df.shape)

train_X, validation_X, train_y, validation_y = train_test_split(train_X_df.to_numpy(), train_y_df.to_numpy(), train_size=0.85, random_state=2021)
print('Sample train features: \n', train_X[:3])
print('\nSample train labels: ', train_y[:3])

print(train_X.shape)
print(validation_X.shape)
print(train_y.shape)
print(validation_y.shape)

"""## k-Nearest Neighbors implementation:

"""

class KNN_Classifier:

    def __init__(self, train_X, train_y, K=5):

        self.K = K
        self.train_X = train_X
        self.train_y = train_y

    #calculates euclidean_distance between 2 datapoints.
    def eucledian(self, point1, point2):
        return np.sqrt(np.sum((point1 - point2) ** 2))

    def get_neighbors(self, current_datapoint):
        dist_from_neighbors = []
        for neighbor_idx, train_row in enumerate (self.train_X):
            # calcualating the eucledian error from current data point to one of the datapoints in train dataset.
            dist = self.eucledian(current_datapoint, train_row)
            dist_from_neighbors.append({'neighbor_index': neighbor_idx, 'dist_value':dist})
        # sorting dist_from_neighbors according to the eucledian error.
        dist_from_neighbors.sort(key=lambda dists: dists['dist_value'])   # key - function that serves as a key for the sort comparison.
        # returning the k-nearest neighbors of current_datapoint using hyperparammeter K. 
        return dist_from_neighbors[:self.K]

    def predict(self, datapoint_X):
        neighbors = self.get_neighbors(datapoint_X)
        # extracting neighbors indexes and mapping them to their corresponding labels in train_y. 
        neighbor_labels = self.train_y[[element['neighbor_index'] for element in neighbors]]
        neighbor_labels = neighbor_labels.astype(np.int32)
         #Majority voting (counting frequent label in k nearest neighbors)
        prediction = np.bincount(neighbor_labels).argmax()
        return prediction

    # this function runs algo on the set of input data(validation or test data).
    def k_nearest_neighbors_run(self, validation_data):
        predictions = []
        for datapoint in validation_data:
            output = self.predict(datapoint)
            predictions.append(output)
        return np.array(predictions)

    # this function calcualtes score(accuracy) of the model.  
    def accuracy_metric(self, actual, predicted):
        correct_count = np.unique((actual == predicted), return_counts=True)[1][1]
        return correct_count / float(actual.shape[0]) * 100.0

"""### KNN Evaluation:"""

knn_model = KNN_Classifier(train_X, train_y, K=3)
print('some prediction on training data:')
for sample_idx in range(25):
    prediction = knn_model.predict(train_X[sample_idx])
    print('Actual label: {},  Predicted label: {}'.format(train_y[sample_idx], prediction))

predictions = knn_model.k_nearest_neighbors_run(train_X)
print('Accuracy on train data: ', knn_model.accuracy_metric(train_y, predictions))

predictions = knn_model.k_nearest_neighbors_run(validation_X)
print('Actual validation labels: \n', validation_y[:10])
print('\nPredicted validation labels: \n', predictions[:10])

print('\nAccuracy on validation data: ', knn_model.accuracy_metric(validation_y, predictions))

print('confusion matrix for validation data KNN model: \n', confusion_matrix(validation_y, predictions))
print('\n', classification_report(validation_y, predictions))

"""## SVM implementation:

### implemented using linear kernel:
"""

from sklearn import svm

# training svm model on training data (with linear kernel.)
svm_model = svm.SVC(kernel='linear')
svm_model.fit(train_X, train_y)     # fit function trains model (updates model's weights)

y_pred_validation = svm_model.predict(validation_X)
print('Actual validation data:\n',validation_y[:10])
print('\nPredicted on validation data:\n', y_pred_validation[:10])
print("\nAccuracy on validation data:",metrics.accuracy_score(validation_y, y_pred_validation) * 100)

print('confusion matrix for validation data: \n', confusion_matrix(validation_y, y_pred_validation))
print('\n', classification_report(validation_y, y_pred_validation))

"""As we can see the score of validation data is 1 (100 percent) for SVM model and is little bit tricky so we can use cross validation to check the performance of alogritem in detail."""

cv = cross_val_score(svm_model,validation_X, validation_y, cv=5) # 5 fold cross validation on SVM with linear kernel. 
print(cv)
print('\ncross validation mean score: ', cv.mean())

"""### Implemented using RBF kernel:"""

# training svm model on training data (with RBF kernel.)
svm_model_RBF = svm.SVC(kernel='rbf')
svm_model_RBF.fit(train_X, train_y)

y_pred_validation = svm_model_RBF.predict(validation_X)
print('Actual validation data:\n',validation_y[:10])
print('\nPredicted on validation data:\n', y_pred_validation[:10])
print("\nAccuracy on validation data:",metrics.accuracy_score(validation_y, y_pred_validation) * 100)

print('confusion matrix for validation data: \n', confusion_matrix(validation_y, y_pred_validation))
print('\n', classification_report(validation_y, y_pred_validation))

"""## Evaluation of trained models on test data:

first we must load test data and apply preprocesses on it.
"""

test_df = pd.read_csv('/content/CIHW2_Datasets/test.csv')
print(test_df.shape)

test_df = test_df.drop(['Cabin', 'PassengerId', 'Name', 'Ticket'], axis = 1)
test_df['Age'] = test_df['Age'].replace(np.NaN, test_df['Age'].mean() )
test_df['Fare'] = test_df['Fare'].replace(np.NaN, test_df['Fare'].mean() )
test_df['Embarked'] = test_df['Embarked'].fillna('S')

test_df['Embarked'] = test_df['Embarked'].map(Embarked_encodings)
test_df['Sex'] = test_df['Sex'].map(gender_encodings) 

test_df.loc[ test_df['Age'] <= 19, 'Age'] = 0
test_df.loc[(test_df['Age'] > 19) & (test_df['Age'] <= 25), 'Age'] = 1
test_df.loc[(test_df['Age'] > 25) & (test_df['Age'] <= 31.8), 'Age'] = 2
test_df.loc[(test_df['Age'] > 31.8) & (test_df['Age'] <= 41), 'Age'] = 3
test_df.loc[test_df['Age'] > 41, 'Age'] = 4

print('shape of preprocessed dataset: ', test_df.shape)
test_df.head(10)

print('\nmissing values:\n', train_df.isnull().sum())

test_X = test_df.to_numpy()
print(test_X[:5])

"""### Evaluation of KNN and SVM models using test data:"""

test_predictions = knn_model.k_nearest_neighbors_run(test_X)
print('\nPredicted test data labels (KNN Model): \n', test_predictions[:25])

y_pred_test = svm_model.predict(test_X)
print('\nPredicted test data labels (SVM with linear kernel): \n', y_pred_test[:25])

y_pred_test_RBF = svm_model_RBF.predict(test_X)
print('\nPredicted test data labels (SVM with RBF kernel): \n', y_pred_test_RBF[:25])

"""## Saving Predictions in .csv file:"""

np.savetxt("KNN_Predictions.csv", test_predictions, delimiter=",")
np.savetxt("SVM_linear_Predictions.csv", y_pred_test, delimiter=",")
np.savetxt("SVM_RBF_Predictions.csv", y_pred_test_RBF, delimiter=",")